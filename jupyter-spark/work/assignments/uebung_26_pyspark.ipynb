{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2da143a",
   "metadata": {},
   "source": [
    "# Analyzing Distributed Data with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56cf749",
   "metadata": {},
   "source": [
    "## Initializing Spark Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1b677db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.functions import to_timestamp, to_date, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f1a23ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session & context\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('spark://spark-master:7077') \\\n",
    "    .appName(\"uebung_26\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b152c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a text file and inspect it\n",
    "def read_csv_file(path, sep=',', header=True):\n",
    "    df = spark.read.csv(\n",
    "        path=path, \n",
    "        sep=sep,\n",
    "        header=header,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    df.printSchema()\n",
    "    df.show(10)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9baa915",
   "metadata": {},
   "source": [
    "## Reading in all relevant files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9c2f899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datum_holi: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- locale: string (nullable = true)\n",
      " |-- locale_name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- transferred: boolean (nullable = true)\n",
      "\n",
      "+----------+-------+--------+-----------+--------------------+-----------+\n",
      "|datum_holi|   type|  locale|locale_name|         description|transferred|\n",
      "+----------+-------+--------+-----------+--------------------+-----------+\n",
      "|2012-03-02|Holiday|   Local|      Manta|  Fundacion de Manta|      false|\n",
      "|2012-04-01|Holiday|Regional|   Cotopaxi|Provincializacion...|      false|\n",
      "|2012-04-12|Holiday|   Local|     Cuenca| Fundacion de Cuenca|      false|\n",
      "|2012-04-14|Holiday|   Local|   Libertad|Cantonizacion de ...|      false|\n",
      "|2012-04-21|Holiday|   Local|   Riobamba|Cantonizacion de ...|      false|\n",
      "|2012-05-12|Holiday|   Local|       Puyo|Cantonizacion del...|      false|\n",
      "|2012-06-23|Holiday|   Local|   Guaranda|Cantonizacion de ...|      false|\n",
      "|2012-06-25|Holiday|Regional|   Imbabura|Provincializacion...|      false|\n",
      "|2012-06-25|Holiday|   Local|  Latacunga|Cantonizacion de ...|      false|\n",
      "|2012-06-25|Holiday|   Local|    Machala|Fundacion de Machala|      false|\n",
      "+----------+-------+--------+-----------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "holidays = read_csv_file('hdfs://namenode:8020/user/root/workspace/pyspark/holidays_events.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "077a7099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_nbr_item: integer (nullable = true)\n",
      " |-- family: string (nullable = true)\n",
      " |-- class: integer (nullable = true)\n",
      " |-- perishable: integer (nullable = true)\n",
      "\n",
      "+-------------+------------+-----+----------+\n",
      "|item_nbr_item|      family|class|perishable|\n",
      "+-------------+------------+-----+----------+\n",
      "|        96995|   GROCERY I| 1093|         0|\n",
      "|        99197|   GROCERY I| 1067|         0|\n",
      "|       103501|    CLEANING| 3008|         0|\n",
      "|       103520|   GROCERY I| 1028|         0|\n",
      "|       103665|BREAD/BAKERY| 2712|         1|\n",
      "|       105574|   GROCERY I| 1045|         0|\n",
      "|       105575|   GROCERY I| 1045|         0|\n",
      "|       105576|   GROCERY I| 1045|         0|\n",
      "|       105577|   GROCERY I| 1045|         0|\n",
      "|       105693|   GROCERY I| 1034|         0|\n",
      "+-------------+------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items = read_csv_file('hdfs://namenode:8020/user/root/workspace/pyspark/items.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3fe9e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date_quito: string (nullable = true)\n",
      " |-- store_nbr_quito: integer (nullable = true)\n",
      " |-- item_nbr_quito: integer (nullable = true)\n",
      " |-- unit_sales: double (nullable = true)\n",
      " |-- onpromotion: boolean (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- cluster: integer (nullable = true)\n",
      "\n",
      "+--------+----------+---------------+--------------+----------+-----------+-----+---------+-------+\n",
      "|      id|date_quito|store_nbr_quito|item_nbr_quito|unit_sales|onpromotion| city|    state|cluster|\n",
      "+--------+----------+---------------+--------------+----------+-----------+-----+---------+-------+\n",
      "|88211471|2016-08-16|             44|        103520|       7.0|       true|Quito|Pichincha|      5|\n",
      "|88211472|2016-08-16|             44|        103665|       7.0|      false|Quito|Pichincha|      5|\n",
      "|88211473|2016-08-16|             44|        105574|      13.0|      false|Quito|Pichincha|      5|\n",
      "|88211474|2016-08-16|             44|        105575|      18.0|      false|Quito|Pichincha|      5|\n",
      "|88211475|2016-08-16|             44|        105577|       8.0|      false|Quito|Pichincha|      5|\n",
      "|88211476|2016-08-16|             44|        105693|       8.0|      false|Quito|Pichincha|      5|\n",
      "|88211477|2016-08-16|             44|        105737|       7.0|      false|Quito|Pichincha|      5|\n",
      "|88211478|2016-08-16|             44|        105857|      27.0|      false|Quito|Pichincha|      5|\n",
      "|88211479|2016-08-16|             44|        106716|       5.0|      false|Quito|Pichincha|      5|\n",
      "|88211480|2016-08-16|             44|        108634|       2.0|      false|Quito|Pichincha|      5|\n",
      "+--------+----------+---------------+--------------+----------+-----------+-----+---------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stores = read_csv_file('hdfs://namenode:8020/user/root/workspace/pyspark/quito_stores_sample2016-2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23acefba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date_trans: string (nullable = true)\n",
      " |-- store_nbr_trans: integer (nullable = true)\n",
      " |-- transactions: integer (nullable = true)\n",
      "\n",
      "+----------+---------------+------------+\n",
      "|date_trans|store_nbr_trans|transactions|\n",
      "+----------+---------------+------------+\n",
      "|2013-01-01|             25|         770|\n",
      "|2013-01-02|              1|        2111|\n",
      "|2013-01-02|              2|        2358|\n",
      "|2013-01-02|              3|        3487|\n",
      "|2013-01-02|              4|        1922|\n",
      "|2013-01-02|              5|        1903|\n",
      "|2013-01-02|              6|        2143|\n",
      "|2013-01-02|              7|        1874|\n",
      "|2013-01-02|              8|        3250|\n",
      "|2013-01-02|              9|        2940|\n",
      "+----------+---------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions = read_csv_file('hdfs://namenode:8020/user/root/workspace/pyspark/transactions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937a8780",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Sum of `unit_sales` of the year 2017 grouped by `item_nbr_quito`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9bb0f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------------+--------------+----------+-----------+-----+---------+-------+----+\n",
      "|      id|date_quito|store_nbr_quito|item_nbr_quito|unit_sales|onpromotion| city|    state|cluster|year|\n",
      "+--------+----------+---------------+--------------+----------+-----------+-----+---------+-------+----+\n",
      "|88211471|2016-08-16|             44|        103520|       7.0|       true|Quito|Pichincha|      5|2016|\n",
      "|88211472|2016-08-16|             44|        103665|       7.0|      false|Quito|Pichincha|      5|2016|\n",
      "|88211473|2016-08-16|             44|        105574|      13.0|      false|Quito|Pichincha|      5|2016|\n",
      "|88211474|2016-08-16|             44|        105575|      18.0|      false|Quito|Pichincha|      5|2016|\n",
      "|88211475|2016-08-16|             44|        105577|       8.0|      false|Quito|Pichincha|      5|2016|\n",
      "+--------+----------+---------------+--------------+----------+-----------+-----+---------+-------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date_quito: date (nullable = true)\n",
      " |-- store_nbr_quito: integer (nullable = true)\n",
      " |-- item_nbr_quito: integer (nullable = true)\n",
      " |-- unit_sales: double (nullable = true)\n",
      " |-- onpromotion: boolean (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- cluster: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Extract the year from date_quito\n",
    "unit_sales = stores.withColumn('date_quito', to_date('date_quito', 'yyyy-MM-dd'))\n",
    "unit_sales = unit_sales.withColumn('year', year('date_quito'))\n",
    "print(unit_sales.show(5))\n",
    "print(unit_sales.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f70bbfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------------+--------------+----------+-----------+----+-----+-------+----+\n",
      "| id|date_quito|store_nbr_quito|item_nbr_quito|unit_sales|onpromotion|city|state|cluster|year|\n",
      "+---+----------+---------------+--------------+----------+-----------+----+-----+-------+----+\n",
      "+---+----------+---------------+--------------+----------+-----------+----+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a subset of year 2017, group it by item_nbr_quito and calculate the sum\n",
    "unit_sales_year = unit_sales.filter(unit_sales['year'] == 2017)\n",
    "unit_sales_year.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb40a0d",
   "metadata": {},
   "source": [
    "Since there are no entries for the year 2017, I will use the year 2016 instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "935f7800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+\n",
      "|item_nbr_quito|sum(unit_sales)|\n",
      "+--------------+---------------+\n",
      "|        454593|         1147.0|\n",
      "|        459762|         3964.0|\n",
      "|        692531|         2729.0|\n",
      "|        699703|      17312.817|\n",
      "|        759651|         2446.0|\n",
      "|        867850|         1209.0|\n",
      "|       1047786|        11074.0|\n",
      "|       1118691|         2159.0|\n",
      "|       1230417|         2220.0|\n",
      "|       1471462|         1600.0|\n",
      "+--------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unit_sales_year = unit_sales.filter(unit_sales['year'] == 2016)\n",
    "unit_sales_year = unit_sales_year.groupBy('item_nbr_quito').sum('unit_sales')\n",
    "unit_sales_year.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d96d2",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Calculate the number of items per family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b4a59576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|       family|count|\n",
      "+-------------+-----+\n",
      "|    GROCERY I| 1334|\n",
      "|    BEVERAGES|  613|\n",
      "|     CLEANING|  446|\n",
      "|      PRODUCE|  306|\n",
      "|        DAIRY|  242|\n",
      "|PERSONAL CARE|  153|\n",
      "| BREAD/BAKERY|  134|\n",
      "|    HOME CARE|  108|\n",
      "|         DELI|   91|\n",
      "|        MEATS|   84|\n",
      "+-------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "family = items.groupBy('family').count().orderBy('count', ascending=False)\n",
    "family.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55efe2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEVER FORGET to stop the session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb9a5c38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method alias in module pyspark.sql.column:\n",
      "\n",
      "alias(*alias, **kwargs) method of pyspark.sql.column.Column instance\n",
      "    Returns this column aliased with a new name or names (in the case of expressions that\n",
      "    return more than one column, such as explode).\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    alias : str\n",
      "        desired column names (collects all positional arguments passed)\n",
      "    \n",
      "    Other Parameters\n",
      "    ----------------\n",
      "    metadata: dict\n",
      "        a dict of information to be stored in ``metadata`` attribute of the\n",
      "        corresponding :class:`StructField <pyspark.sql.types.StructField>` (optional, keyword\n",
      "        only argument)\n",
      "    \n",
      "        .. versionchanged:: 2.2.0\n",
      "           Added optional ``metadata`` argument.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.select(df.age.alias(\"age2\")).collect()\n",
      "    [Row(age2=2), Row(age2=5)]\n",
      "    >>> df.select(df.age.alias(\"age3\", metadata={'max': 99})).schema['age3'].metadata['max']\n",
      "    99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(to_date(stores['date_quito'], 'yyyy-MM-dd').alias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
